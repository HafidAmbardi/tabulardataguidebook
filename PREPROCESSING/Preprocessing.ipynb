{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Columns\n",
    "**Use Case:** Shortening column names, fixing typos, removing spaces, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = df.rename(columns={'old_name': 'new_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns\n",
    "**Use Case:**\n",
    "Removing unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = df.drop(columns=['column_to_drop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "**Use Case:** \\\n",
    "To increase code simplicity, Pipelines are used. \\\n",
    "Pipelines allow you to conduct many steps such as preprocessing \\\n",
    "in minimal amounts of code.\n",
    "\n",
    "**Code Logic:** \\\n",
    "First, define the steps of your pipeline (ensure each step is compatible with pipelines. \\\n",
    "Second, create the pipeline with the aformentioned steps. \\\n",
    "Thirdly, use the pipeline. Common methods are fit, transform, fit_transform, and predict.\n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "Feature selection \\\n",
    "Model Evaluation \\\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps of the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),  # Transformer\n",
    "    ('model', LogisticRegression())  # Estimator\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Use the pipeline to fit and predict\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformer Pipelines\n",
    "**Use Case:** \\\n",
    "Create your own preprocessing pipelnes when you have a transformation \\\n",
    "unique to your dataset (e.g. string transformations)\n",
    "\n",
    "**Code Logic:** \\\n",
    "First, define a class name and pass BaseEstimator (for parameter tuning) \\\n",
    "and TransformerMixin (for transform & fit methods) \\\n",
    "Second, define the constructor (\\_\\_init\\_\\_) \\\n",
    "Third, define the fit method\n",
    "Fourth, define the transform method\n",
    "\n",
    "Afterwards your Pipeline class can be added to steps similar to the previous code cell. \\ \n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "Feature selection \\\n",
    "Model Evaluation \\\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, param1):\n",
    "        self.param1 = param1\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # This should return self unless something different happens in train and test\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Depending on the data type of 'X', you might need to return a DataFrame, a Series or a numpy array\n",
    "        X_transformed = X.copy()  # creating a copy to avoid changes to original dataset\n",
    "        X_transformed = X_transformed + self.param1  # an example operation using 'param1'\n",
    "        return X_transformed\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('custom', CustomTransformer(param1=value)),\n",
    "    # ... other steps in the pipeline ...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV with Pipelines\n",
    "**Use Case:** \\\n",
    "This allows you to use Cross Validation alongside preprocessing that prevents data leakage. \\\n",
    "Moreover, you can hyper parameter tune each step of the Pipeline.\n",
    "\n",
    "**Code Logic:** \\\n",
    "First, initialize a dictionary with the name of the parameters you want to tune. \\\n",
    "Second, pass the pipeline into GridSearchCV. \\\n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "Feature selection \\\n",
    "Model Evaluation \\\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps of the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),  # Transformer\n",
    "    ('model', LogisticRegression())  # Estimator\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'model__C': [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
