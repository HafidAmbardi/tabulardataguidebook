{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Renaming Columns\n",
    "**Use Case:** Shortening column names, fixing typos, removing spaces, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = df.rename(columns={'old_name': 'new_name'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropping Columns\n",
    "**Use Case:**\n",
    "Removing unnecessary columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = df.drop(columns=['column_to_drop'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines\n",
    "**Use Case:** \\\n",
    "To increase code simplicity, Pipelines are used. \\\n",
    "Pipelines allow you to conduct many steps such as preprocessing \\\n",
    "in minimal amounts of code.\n",
    "\n",
    "**Code Logic:** \\\n",
    "First, define the steps of your pipeline (ensure each step is compatible with pipelines. \\\n",
    "Second, create the pipeline with the aformentioned steps. \\\n",
    "Thirdly, use the pipeline. Common methods are fit, transform, fit_transform, and predict.\n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "Feature selection \\\n",
    "Model Evaluation \\\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps of the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),  # Transformer\n",
    "    ('model', LogisticRegression())  # Estimator\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Use the pipeline to fit and predict\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "pipeline.fit(X_train, y_train)\n",
    "predictions = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Transformer Pipelines\n",
    "**Use Case:** \\\n",
    "Create your own preprocessing pipelnes when you have a transformation \\\n",
    "unique to your dataset (e.g. string transformations)\n",
    "\n",
    "**Code Logic:** \\\n",
    "First, define a class name and pass BaseEstimator (for parameter tuning) \\\n",
    "and TransformerMixin (for transform & fit methods) \\\n",
    "Second, define the constructor (\\_\\_init\\_\\_) \\\n",
    "Third, define the fit method\n",
    "Fourth, define the transform method\n",
    "\n",
    "Afterwards your Pipeline class can be added to steps similar to the previous code cell. \\ \n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "Feature selection \\\n",
    "Model Evaluation \\\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class CustomTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, param1):\n",
    "        self.param1 = param1\n",
    "\n",
    "    def fit(self, X, y=None):\n",
    "        # This should return self unless something different happens in train and test\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        # Depending on the data type of 'X', you might need to return a DataFrame, a Series or a numpy array\n",
    "        X_transformed = X.copy()  # creating a copy to avoid changes to original dataset\n",
    "        X_transformed = X_transformed + self.param1  # an example operation using 'param1'\n",
    "        return X_transformed\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('custom', CustomTransformer(param1=value)),\n",
    "    # ... other steps in the pipeline ...\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV with Pipelines\n",
    "**Use Case:** \\\n",
    "This allows you to use Cross Validation alongside preprocessing that prevents data leakage. \\\n",
    "Moreover, you can hyper parameter tune each step of the Pipeline.\n",
    "\n",
    "**Code Logic:** \\\n",
    "First, initialize a dictionary with the name of the parameters you want to tune. \\\n",
    "Second, pass the pipeline into GridSearchCV. \\\n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "Feature selection \\\n",
    "Model Evaluation \\\n",
    "Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Define the steps of the pipeline\n",
    "steps = [\n",
    "    ('scaler', StandardScaler()),  # Transformer\n",
    "    ('model', LogisticRegression())  # Estimator\n",
    "]\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(steps)\n",
    "\n",
    "# Define the parameter grid for the grid search\n",
    "param_grid = {\n",
    "    'scaler__with_mean': [True, False],\n",
    "    'model__C': [0.1, 1.0, 10.0],\n",
    "}\n",
    "\n",
    "# Create the grid search object\n",
    "grid_search = GridSearchCV(pipeline, param_grid, cv=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KBinsDiscretizer\n",
    "**Use Case:** \\\n",
    "Good for reducing noise, handling outliers, or improving model preformance/simplicity.\n",
    "\n",
    "**Code Logic:**\n",
    "* columns_to_bin should be initialized with a list of column names you want to discretize.\n",
    "* adjust the parameters of KBinsDiscretizer as needed\n",
    "* perform fit_transform on training data and transform on test data\n",
    "* lastly, you can print the edges from fit_transform. However, it is not strictly needed to perform discretization\n",
    "\n",
    "**Actionable Next Steps:** \\\n",
    "EDA \\\n",
    "Feature Engineering \\\n",
    "Modeling \n",
    "\n",
    "**Notes:** \\\n",
    "[Full documentation](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.KBinsDiscretizer.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[23.  31.8 40.6 49.4 58.2 67. ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "X1 = df1.copy()\n",
    "X2 = df2.copy()\n",
    "\n",
    "# Define the columns you want to bin\n",
    "columns_to_bin = ['Age']\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', ColumnTransformer(transformers=[\n",
    "        ('discretizer', KBinsDiscretizer(n_bins=5, encode='ordinal', strategy='uniform'), columns_to_bin)\n",
    "    ])),\n",
    "])\n",
    "\n",
    "X1_transformed = pipeline.fit_transform(X1)\n",
    "X2_transformed = pipeline.transform(X2)\n",
    "\n",
    "# Fit the pipeline\n",
    "X1['Age'] = pd.DataFrame(X1_transformed, columns=columns_to_bin)\n",
    "X2['Age'] = pd.DataFrame(X2_transformed, columns=columns_to_bin)\n",
    "\n",
    "# Access the 'preprocessor' step in the pipeline\n",
    "preprocessor = pipeline.named_steps['preprocessor']\n",
    "\n",
    "# Access the 'discretizer' transformer in the preprocessor\n",
    "discretizer = preprocessor.named_transformers_['discretizer']\n",
    "\n",
    "# Print the bin edges\n",
    "print(discretizer.bin_edges_[0])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
